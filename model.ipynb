{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237af4d4",
   "metadata": {},
   "source": [
    "## Задача - предсказать число книг на фотографии объявления\n",
    "### Общее решение - будем использовать готовый детектор объектов с дополнительной фильтрацией, из которого будем считать число объектов с лейблом \"book\".\n",
    "Почему не обучить свою модель? **Данные нельзя размечать**, поэтому для дообучения модели нужно сгенерировать хорошие синтетические данные. Я пробовал генерировать обложки книг и фонов с помощью Stable Diffusion 2, но результат очень слабо похож на реальные данные. К тому же, с учетом лимита по времени, намного проще и быстрее улучшить работу готового детектора, нежели дообучать модель.\n",
    "### Общий пайплайн:\n",
    "1. Изображение препроцессится - увеличивает контраст, убираем шум.\n",
    "2. Изображение отправляется в детектор объектов D-Fine https://github.com/Peterande/D-FINE (ICLR Spotlight 2025 и совсем недавно добавили на HF). \n",
    "Выбор модели основан на ее быстрого инференса, а также хорошего качества предсказания и быстрой установки из HF. Модель предсказывает все объекты с порогом 0.\n",
    "3. Изображение также отправляется в CLIP для дополнительной проверки наличия книг на фото. CLIP оценивает вероятность наличия книги на изображении.\n",
    "4. Полученные предсказания от детектора и CLIP переходят в count_from_raw(), где происходит фильтрация по порогу. \n",
    "    a) Оставляем только объекты с лейблом книга.\n",
    "    b) Если вероятность нахождения книг на картинке из CLIP меньше 0.05, то сразу возвращаем что число книг равно 0. Это делается как начальная фильтрация, в случаях когда детектор находит книги, где их нет.\n",
    "    с) Фильтруем по верхнему порогу для детектора 0.6. Если нету минимум 3 объектов с таким скором, то понижаем его до нижней границы 0.4. Это делается специально для случаев, когда есть много книг, или целая стопка. В таком случае у детектора несколько ниже скоры для всех книг, и таким образом мы адаптивно понижаем порог. В случае с одной книгой, обычно есть только один большой скор, поэтому понижение порога ничего не меняет.  \n",
    "    d) Если в результате фильтрации по результатам предсказаний детектора ничего нету, но CLIP говорит иначе, то как финальная проверка понижаем порог до 0.15 для детектора и пробуем найти книги.\n",
    "    e) Наконец проводим NMS чтобы удалить избыточные пересекающиеся bboxы\n",
    "    f) Возвращаем оставшиеся число объектов на картинке\n",
    "\n",
    "### Что пробовал, но не сработало:\n",
    "* Создание синтетического датасета с Stable Diffusion 2\n",
    "* Детекция объектов по тайлам изображения\n",
    "* Отдельная проверка на существование книги на каждом bboxe с помощью CLIP\n",
    "* Ансамбль нескольких детекторов\n",
    "\n",
    "### Результаты:\n",
    "Модель предсказывает число книг с balanced accuracy ~50%. Метрика довольно жестокая, так как на паре примеров 20 книгами модель может ошибиться на 1-2 книги и все равно ответ будет считаться полностью неверным. На большинстве случаев (1 книга или 0 книг) модель показывает очень хорошие результаты. Тем не менее, результат можно улучшить, например, созданием нормального размеченного датасета и дообучения модели детектора на конкретных примерах изображений с книгами. Также можно использовать фичи детектора для обучения простого регрессора, нежели использовать число объектов напрямую. Можно улучшить фильтрацию изображений заменой CLIP на современные VLM модели, и возможно даже получать в качестве предсказаний конкретные числа книг, нежели их наличие/отсутствие. На этапе предсказания можно попробовать аугментации изображений во время инференса, так как некоторые изображения были изначально перевернуты, и поэтому могли быть не корректно анализированы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f15e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from triton==3.4.0->torch) (78.1.1)\n",
      "Requirement already satisfied: numpy in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: numpy in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (2.2.6)\n",
      "Requirement already satisfied: pandas in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (2.3.2)\n",
      "Requirement already satisfied: tqdm in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: pillow in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (11.3.0)\n",
      "Requirement already satisfied: scikit-learn in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (1.7.0)\n",
      "Requirement already satisfied: matplotlib in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (3.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: transformers in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (4.53.1)\n",
      "Requirement already satisfied: filelock in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/28s_mur@lab.graphicon.ru/miniconda3/envs/attack/lib/python3.11/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install numpy pandas tqdm pillow scikit-learn matplotlib opencv-python\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1613419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image, ImageEnhance\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from torchvision.ops import nms\n",
    "from transformers import AutoProcessor, DFineForObjectDetection, CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879754dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dataset_folder = \"task_images\"\n",
    "\n",
    "# CLIP params\n",
    "CLIP_MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "CLIP_TEXTS = [\"a photo of a book\", \"no book\"]\n",
    "CLIP_PRESENCE_TH = 0.6\n",
    "\n",
    "# detector params\n",
    "detector_id = \"ustc-community/dfine-xlarge-coco\"\n",
    "DETECTOR_THRESH = 0.6\n",
    "DETECTOR_THRESH_LOW = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c118afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector wrapper\n",
    "class DFineWrapper:\n",
    "    def __init__(self, model_id):\n",
    "        self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "        self.model = DFineForObjectDetection.from_pretrained(model_id).to(device)\n",
    "        self.id2label = self.model.config.id2label\n",
    "        inv = {v: k for k, v in self.id2label.items()}\n",
    "        self.book_label_id = inv.get(\"book\", None)\n",
    "\n",
    "    # generate bboxes, scores and labels for given image with a given minimal score threshold \n",
    "    def predict_raw(self, pil_img, min_threshold=0.0):\n",
    "        w, h = pil_img.size\n",
    "        inputs = self.processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        target_sizes = [(h, w)]\n",
    "        preds = self.processor.post_process_object_detection(\n",
    "            outputs, target_sizes=target_sizes, threshold=min_threshold\n",
    "        )\n",
    "        if len(preds) == 0:\n",
    "            return np.zeros((0, 4)), np.zeros(0), np.zeros(0, dtype=int)\n",
    "        p = preds[0]\n",
    "        boxes = p.get(\"boxes\", torch.zeros((0, 4))).cpu().numpy()\n",
    "        scores = p.get(\"scores\", torch.zeros(0)).cpu().numpy()\n",
    "        labels = p.get(\"labels\", torch.zeros(0)).cpu().numpy().astype(int)\n",
    "        return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# preprocess image before running models\n",
    "def preprocess_image(image):\n",
    "    np_img = np.array(image)\n",
    "\n",
    "    # fast denoise\n",
    "    np_img = cv2.fastNlMeansDenoisingColored(np_img, None, 10, 10, 7, 15)\n",
    "    # image = ImageEnhance.Brightness(image).enhance(1.5)\n",
    "    \n",
    "    # increase contrast\n",
    "    image = ImageEnhance.Contrast(image).enhance(1.2)\n",
    "    return image\n",
    "\n",
    "# perform nms to remove overlapping bboxes\n",
    "def perform_nms_np(boxes, scores, iou_thresh):\n",
    "    if boxes.shape[0] == 0:\n",
    "        return np.array([], dtype=int)\n",
    "    boxes_t = torch.from_numpy(boxes).float()\n",
    "    scores_t = torch.from_numpy(scores).float()\n",
    "    keep = nms(boxes_t, scores_t, iou_thresh).cpu().numpy()\n",
    "    return keep\n",
    "\n",
    "# function for running predictions on CLIP\n",
    "def clip_image_book_prob(pil_img, clip_processor, clip_model):\n",
    "    inputs = clip_processor(\n",
    "        text=CLIP_TEXTS, images=pil_img, return_tensors=\"pt\", padding=True\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = clip_model(**inputs)\n",
    "    probs = torch.softmax(out.logits_per_image, dim=1).cpu().numpy()[0]\n",
    "    # find logit index of a book being on a photo\n",
    "    try:\n",
    "        idx = CLIP_TEXTS.index(\"a photo of a book\")\n",
    "    except ValueError:\n",
    "        raise RuntimeError(f\"'book' not found in CLIP_TEXTS: {CLIP_TEXTS}\")\n",
    "    \n",
    "    # return confidence of book being on a photo\n",
    "    return float(probs[idx])\n",
    "\n",
    "# main function for running detector and CLIP on every image\n",
    "def build_predictions_dataset(wrapper, clip_processor, clip_model, folder, min_threshold=0.0, max_images=None, visualize_n=3):\n",
    "    \n",
    "    # get sorted paths of images \n",
    "    paths = sorted(\n",
    "        glob(os.path.join(folder, \"*.jpg\")),\n",
    "        key=lambda x: int(os.path.splitext(os.path.basename(x))[0]),\n",
    "    )\n",
    "    if max_images:\n",
    "        paths = paths[:max_images]\n",
    "\n",
    "    preds = []\n",
    "    for i, p in enumerate(tqdm(paths)):\n",
    "        img_id = int(os.path.splitext(os.path.basename(p))[0])\n",
    "        pil = preprocess_image(Image.open(p).convert(\"RGB\"))\n",
    "        \n",
    "        # run predictions for CLIP model and detector\n",
    "        clip_prob = clip_image_book_prob(pil, clip_processor, clip_model)\n",
    "        boxes, scores, labels = wrapper.predict_raw(pil, min_threshold)\n",
    "        \n",
    "        # add prediction values\n",
    "        preds.append(\n",
    "            {\n",
    "                \"image_id\": img_id,\n",
    "                \"boxes\": boxes,\n",
    "                \"scores\": scores,\n",
    "                \"labels\": labels,\n",
    "                \"clip_prob\": clip_prob,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # if i < visualize_n:\n",
    "        #     plt.figure(figsize=(6,6))\n",
    "        #     plt.imshow(pil)\n",
    "        #     plt.axis(\"off\")\n",
    "        #     plt.title(f\"id {img_id} | CLIP(book)={clip_prob:.2f}\", fontsize=14)\n",
    "        #     plt.show()\n",
    "\n",
    "    return preds\n",
    "\n",
    "# main function for filtering bboxes and returning book amount\n",
    "def count_from_raw(\n",
    "    boxes,\n",
    "    scores,\n",
    "    labels,\n",
    "    wrapper_book_id,\n",
    "    high_th=0.6,\n",
    "    low_th=0.4,\n",
    "    min_high=3,\n",
    "    apply_nms=True,\n",
    "    nms_iou=0.5,\n",
    "    clip_prob=None,\n",
    "    clip_presence_th=CLIP_PRESENCE_TH,\n",
    "    fallback_low_th=0.15,\n",
    "    clip_absent_th=0.05,  \n",
    "):\n",
    "    # mask out all bboxes with the label \"book\"\n",
    "    if wrapper_book_id is not None:\n",
    "        mask = labels == wrapper_book_id\n",
    "    else:\n",
    "        mask = np.ones_like(labels, dtype=bool)\n",
    "\n",
    "    boxes = boxes[mask]\n",
    "    scores = scores[mask]\n",
    "\n",
    "    # if CLIP confidence score for an image is too low, then the image is automatically considered without the books\n",
    "    if clip_prob is not None and clip_prob < clip_absent_th:\n",
    "        return 0\n",
    "\n",
    "    if boxes.shape[0] == 0:\n",
    "        return 0\n",
    "\n",
    "    # basic score threshold for bboxes\n",
    "    high_mask = scores > high_th\n",
    "    \n",
    "    # if the amount of highly confident predictions is too low, then we lower our threshold\n",
    "    if high_mask.sum() >= min_high:\n",
    "        final_mask = high_mask\n",
    "    else:\n",
    "        final_mask = scores > low_th\n",
    "\n",
    "    boxes_f = boxes[final_mask]\n",
    "    scores_f = scores[final_mask]\n",
    "\n",
    "    # if based on the detector threshold there are no books, but CLIP says otherwise we try again and lower score threshold \n",
    "    if boxes_f.shape[0] == 0 and (clip_prob is not None and clip_prob >= clip_presence_th):\n",
    "        retry_mask = scores > fallback_low_th\n",
    "        boxes_f = boxes[retry_mask]\n",
    "        scores_f = scores[retry_mask]\n",
    "\n",
    "    if boxes_f.shape[0] == 0:\n",
    "        return 0\n",
    "\n",
    "    # perform nms on resulting bboxes to remove clearly overlapping ones\n",
    "    if apply_nms:\n",
    "        keep = perform_nms_np(boxes_f, scores_f, nms_iou)\n",
    "        return int(len(keep))\n",
    "    else:\n",
    "        return int(len(boxes_f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87273593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2334fab46e54b5fa142e432e3dc8d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved solution.csv\n"
     ]
    }
   ],
   "source": [
    "# run model\n",
    "\n",
    "# init CLIP\n",
    "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_ID)\n",
    "clip_model = CLIPModel.from_pretrained(CLIP_MODEL_ID).to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "#init DFine detector\n",
    "wrapper = DFineWrapper(detector_id)\n",
    "\n",
    "# create bbox predictions on all images\n",
    "preds = build_predictions_dataset(wrapper, clip_processor, clip_model, dataset_folder, min_threshold=0.0, max_images=None)\n",
    "image_ids = [p[\"image_id\"] for p in preds]\n",
    "\n",
    "y_pred = []\n",
    "for pred in preds:\n",
    "    # for each image filter out unneeded bboxes based on their scores and count amount of bboxes with label \"book\"\n",
    "    c = count_from_raw(\n",
    "        pred[\"boxes\"],\n",
    "        pred[\"scores\"],\n",
    "        pred[\"labels\"],\n",
    "        wrapper.book_label_id,\n",
    "        clip_prob=pred.get(\"clip_prob\", 0.0),\n",
    "        high_th=DETECTOR_THRESH,\n",
    "        low_th=DETECTOR_THRESH_LOW\n",
    "    )\n",
    "    y_pred.append(c)\n",
    "\n",
    "# save results as csv\n",
    "out_df = pd.DataFrame({\"image_id\": image_ids, \"number_of_books\": y_pred})\n",
    "out_df.to_csv(\"solution.csv\", index=False)\n",
    "print(\"Saved solution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff11b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
